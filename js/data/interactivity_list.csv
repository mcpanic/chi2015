int102,A,Wrist Compression Feedback by Pneumatic Actuation,Henning,Pohl,HenningPohl@gmail.com,"Henning Pohl, Dennis Becke, Eugen Wagner, Maximilian Schrapel, Michael Rohs","Henning@hci.uni-hannover.de, dennis.becke@web.de, eugen_wagner@hotmail.de, maxud@gmx.net, michael.rohs@hci.uni-hannover.de","Most common forms of haptic feedback use vibration, which immediately captures the user's attention, yet is limited in the range of strengths it can achieve. Vibration feedback over extended periods also tends to be annoying. We present compression feedback, a form of haptic feedback that scales from very subtle to very strong and is able to provide sustained stimuli and pressure patterns. The demonstration may serve as an inspiration for further work in this area, applying compression feedback to generate subtle, intimate, as well as intense feedback.",Wrist Compression Feedback by Pneumatic Actuation,Strap in and feel your arm being squeezed. We use pneumatic actuation to generate several different patterns of compression feedback. Experience how this scales from subtle to intense.,,,,
int105,A,Harmonious Haptics: Enhanced Tactile Feedback Using a Mobile and a Wearable Device,Sungjae,Hwang,best@kaist.ac.kr,"Sungjae Hwang, John Song, Junghyeon Gim","Sungjae.hwang@futureplay.co, John.song@futureplay.co, Junghyeon.gim@futureplay.co","Smartwatches now allow information to be conveniently accessed directly from the user’s wrist. However, the smartwatches currently available in the market offer a limited number of applications. In this paper, we propose a new interaction technique named Harmonious Haptics, which provides users with enhanced tactile sensations by utilizing smartwatches as additional tactile displays for smartphones. When combined with typical mobile devices, our technique enables the design of a wide variety of tactile stimuli. To illustrate the potential of our approach, we developed a set of example applications that provide users with rich tactile feedback such as feeling textures in a graphical user interface, transferring a file between the tablet and the smartwatch device, and controlling UI components.",Harmonious Haptics: Enhanced Tactile Feedback Using a Mobile and a Wearable Device,Experience a amazing tactile feedbacks using mobile and wearable devices!,,,,
int108,A,Low-Fidelity Fabrication: Speeding up Design Iteration of 3D Objects,Stefanie,Mueller,stefanie.mueller@student.hpi.uni-potsdam.de,"Stefanie Mueller, Dustin Beyer, Tobias Mohr, Serafima Gurevich, Alexander Teibrich, Lisa Pfisterer, Kerstin Guenther, Johannes Frohnhofen, Hsiang-Ting Chen, Patrick Baudisch, Sangha Im, François V Guimbretière","stefanie.mueller@student.hpi.uni-potsdam.de, dustin.beyer@student.hpi.uni-potsdam.de, tobias.mohr@student.hpi.uni-potsdam.de, Serafima.Gurevich@hpi.uni-potsdam.de, alexander.teibrich@student.hpi.uni-potsdam.de, lisa.pfisterer@student.hpi.uni-potsdam.de, kerstin.guenther@student.hpi.uni-potsdam.de, johannes.frohnhofen@student.hpi.uni-potsdam.de, ht.timchen@gmail.com, patrick.baudisch@hpi.uni-potsdam.de, si237@cornell.edu, francois@cs.cornell.edu","Low-fidelity fabrication systems speed up rapid prototyping by printing intermediate versions of a prototype as fast, low-fidelity previews. Only the final version is fabricated as a full high-fidelity 3D print. This allows designers to iterate more quickly—achieving a better design in less time.  \  \ Depending on what is currently being tested, low-fidelity fabrication is implemented in different ways: (1) faBrickator allows for a modular approach by substituting sub-volumes of the 3D model with building blocks. (2) WirePrint allows for quickly testing the shape of an object, such as the ergonomic fit, by printing wireframe structures. (3) Platener preserves the technical function by substituting 3D print with laser-cut plates of the same size and thickness.  \  \ At our CHI’15 interactivity booth, we give a combined live demo of all three low-fidelity fabrication systems—putting special focus on our new low-fidelity fabrication system Platener (paper at CHI’15).  \ ",Low-Fidelity Fabrication: Speeding up Design Iteration of 3D Objects,Low-fidelity fabrication systems speed up rapid prototyping by printing intermediate versions of a prototype as fast low-fidelity previews. Only the final version is fabricated as a full high-fidelity 3D print. ,,,,
int109,A,PaperPulse: An Integrated Approach to Fabricating Interactive Paper,Kashyap,Todi,kashyap.todi@uhasselt.be,"Raf Ramakers, Kashyap Todi, Kris Luyten","raf.ramakers@gmail.com, kashyap.todi@uhasselt.be, kris.luyten@uhasselt.be","We present PaperPulse, a design and fabrication approach that enables designers to produce standalone interactive paper artifacts by augmenting them with electronics. With PaperPulse, users overlay visual designs with widgets provided in the design tool. PaperPulse provides three families of widgets, designed for smooth integration with paper, for a total of 20 different interactive components. We also contribute a demonstration and recording approach, Pulsation, that allows specifying interaction logic. Using the final design and the recorded Pulsation logic, PaperPulse generates layered electronic circuit designs, and code that can be deployed on a microcontroller. By following automatically generated assembly instructions, designers can seamlessly integrate the microcontroller and widgets in the final paper artifact.",PaperPulse,"Experience the entire process of designing, printing, and assembling standalone interactive paper artefacts using PaperPulse. No programming or electronics skills required!",,,,
int110,A,Level-Ups: Motorized Stilts that Simulate Stair Steps in Virtual Reality,Dominik,Schmidt,dominik.schmidt@hpi.uni-potsdam.de,"Dominik Schmidt, Robert Kovacs, Vikram Mehta, Udayan Umapathi, Sven Köhler, Lung-Pan Cheng, Patrick Baudisch","dominik.schmidt@hpi.uni-potsdam.de, robert.kovacs@hpi.de, vmehta@mpi-inf.mpg.de, udayan.umapathi@hpi.de, sven.koehler@student.hpi.uni-potsdam.de, lung-pan.cheng@hpi.uni-potsdam.de, patrick.baudisch@hpi.uni-potsdam.de","We present “Level-Ups”, computer-controlled stilts that allow virtual reality users to experience walking up and down steps. Each Level-Up unit is a self-contained device worn like a boot. Its main functional element is a vertical actuation mechanism mounted to the bottom of the boot that extends vertically. Unlike traditional solutions that are integrated with locomotion devices, Level-Ups allow users to walk around freely (“real-walking”). We present Level-Ups in a demo environment based on a head-mounted display, optical motion capture, and integrated with a game engine.",Level-Ups: Motorized Stilts that Simulate Stair Steps in Virtual Reality,"Level-Ups, motorized stilts that extend and collapse controlled by the computer, allow you to experience walking up and down steps in virtual worlds.",,,,
int115,A,Designing Engaging Data in Communities,Alex,Taylor,ast@microsoft.com,"Tim Regan, David Sweeney, John Helmes, Vasillis Vlachokyriakos, Siân Lindley, Alex Taylor","timregan@microsoft.com, dsweeney@microsoft.com, johnhelmes@gmail.com, v.vlachokyriakos@newcastle.ac.uk, sianl@microsoft.com, ast@microsoft.com","We present two sets of ‘data technologies’ that we have designed to collect and display local data, both derived from our engagement with a community. The first, Bull-frog, is a bespoke voting device. The second, a series of physical charts, respond to the increasing sophistica-tion of data visualisations by making playful use of pie charts and bar graphs, reimagining them in mechanical forms that are compelling but easily readable.",Designing Engaging Data in  Communities,"Exhibit showcasing data tools designed for a community project in Cambridge (UK), on Tenison Road. The tools aim to materially engage community in the processes of data production and use.",,,,
int116,A,The EmotiveModeler: An Emotive Form Design CAD Tool,Philippa,Mothersill,pip@mit.edu,"Philippa Mothersill, V. Michael Bove Jr.","pip@mit.edu, vmb@media.mit.edu","Whether or not we are experts in the design language of objects, we have an unconscious understanding of the emotional character of their forms.  The EmotiveModeler integrates knowledge about our emotive perception of shapes into a CAD tool that uses descriptive adjectives as an input to aid designers in creating objects that can communicate emotive character.  Through inputting words into the EmotiveModeler UI in the Rhinoceros 3D modeling software, both expert and novice designers can manipulate the design of a bottle to express emotive character through its form.",The EmotiveModeler: An Emotive Form Design CAD Tool,Discover your inner expressive designer while using the EmotiveModeler CAD tool to manipulate the character of 3D forms using only emotive adjectives.,,MIT Media Lab consortium,,
int117,A,Remnance of Form: Interactive Narratives through Unexpected Behaviors of a Shadow,Sang-won,Leigh,sangwon@mit.edu,"Sang-won Leigh, Asta Roseway, Ann Paradiso, Pattie Maes","sangwon@mit.edu, astar@microsoft.com, annpar@microsoft.com, pattie@media.mit.edu","Remnance of Form is an interactive installation that explores the dynamic tension between an object and its shadow. By fusing light, projection, and motion technologies, the shadow can now detach itself from its former role. This creates a new narrative that challenges our perception of reality, what’s real and what’s not. Through several playful vignettes, the shadow interacts with viewers’ presence, body posture, and their manipulation of the light source creating the shadow.",Remnance of Form,Interactive Narratives through Unexpected Behaviors of a Shadow,,,,
int123,A,Wearable Devices for Enhancing Communications and Activities between the Blind and Ordinary People through a Waltz,Yoonji,Song,yoonji816@gmail.com,"Yoonji Song, Jiye Kim","yoonji816@gmail.com, mingle14@naver.com","This research has begun from recognizing a problem as to activities that arises by limitations of visual information for the blind. Mingle is a wearable device as a communication medium through senses of hearing and touch which helps the blind and ordinary people who have no visual impairment easily and intuitively learn a waltz. The entire device operation coupled with a waltz basic step comprises of 4 phases of a step, basic step, box step and an advanced box step linked to the song. In terms of a modality, hearing information informs of the voice information as to steps and tactual information informs of the directions of the steps that vibration information located in each device teaches. All the information are communicated to the blind and ordinary people in the same way and through this, the blind and ordinary people can learn the dance without any visual reliance.",Mingle,Mingle is a wearable device as a communication medium through senses of hearing and touch which helps the blind and non-blind easily and intuitively learn a waltz. ,,,,
int124,A,EdiPulse: Turning Physical Activity Into Chocolates,Rohit Ashok,Khot,rohit.a.khot@gmail.com,"Rohit Ashok Khot, Ryan Pennings, Florian 'Floyd' Mueller","rohit.a.khot@gmail.com, s3378565@student.rmit.edu.au, floyd@floydmueller.com","We present EdiPulse that creates 3D printed chocolates displaying cheerful messages using the heart rate data of physical activity. Our work expands the view on representing physical activity data through the use of edible materials such as chocolates, which additionally serves as a hedonic reward for doing the physical activity. Ultimately, with this work, we aim to inspire and guide design thinking on food printing, which we believe opens up new interaction possibilities to support the physical activity experience.",EdiPulse: Turning Physical Activity Into Chocolates,Sweat was never so Sweet. Come experience the journey from Sweat to Sweet.,,,,
int128,A,ListenTree: Audio-Haptic Display In The Natural Environment,Edwina,Portocarrero,edwina@media.mit.edu,"Edwina Portocarrero, Gershon Dublon, Joseph Paradiso, V. Michael Bove Jr.","edwina@media.mit.edu, gershon@media.mit.edu, joep@media.mit.edu, vmb@media.mit.edu","In this paper, we present ListenTree, an audio-haptic display embedded in the natural environment. A visitor to our installation notices a faint sound appearing to emerge from a tree, and might feel a slight vibration under their feet as they approach. By resting their head against the tree, they are able to hear sound through bone conduction. To create this effect, an audio exciter transducer is weatherproofed and attached to the tree trunk underground, transforming the tree into a living speaker that channels audio through its branches. Any source of sound can be played through the tree, including live audio or pre-recorded tracks. For example, we used the ListenTree to display live streaming sound from an outdoor ecological monitoring sensor network, bringing an urban audience into contact with a faraway wetland. Our intervention is motivated by a need for forms of display that fade into the background, inviting attention rather than requiring it. ListenTree points to a future where digital information might become a seamless part of the physical world.",ListenTree: Audio-Haptic Display In The Natural Environment ,"Touch and lean your head against the tree to hear \ sound streaming live from a remote place. Vibrations are audible through bone conduction, turning the tree into a telepresence portal.",,,,
int131,A,Know Yourself: Self-portrait with Emotion Expressed in the EEG Data,Suyeon,Kim,kimsuyeon0@gmail.com,"Hyo-jin Kim, Su-yeon Kim","hyonee1031@gmail.com, kimsuyeon0@gmail.com","Self-portrait was painted for the reflection of the inner face of the old days, not only just draw a face of the human beings. So self-portrait is re-interpreted in various ways to adapt the new media platforms that is caused by the development of the media, and different to represent. In this study, our project - “Know yourself"" is not only tried to make an opportunity to express an external description, but also reflected them to draw a self-portrait for using brainwave that include one’s feelings. At first, ""Know yourself” is starting to make a question to the audience to see or not to see. It is a question that one's face reflected in a mirror is spoken real emotion and respected all about them. The technology of our project to draw the self-portrait is a linking methods use an 'EEG Analysis algorithm' express the face of the 'EEG headset' and face recognition processing solutions. We don’t have many chances to remind our feelings because of the many circumstances that surround to you or change suddenly. We sincerely want that user makes an answer to use our self-inner portrait project.",Know yourself (Interactive self portrait used by the brainwave),Draw your self-portrait by using brainwave that include your feelings!,,,,
int132,A,Researcher: A Reading Application Helping the Flow of Research in Tablet and Mobile Phone,Minjeong,Kang,mjmiso@gmail.com,"Minjeong Kang, Juhyun Eune","mjmiso@snu.ac.kr, jheune@snu.ac.kr","With a growing digital environment, a huge quantity of digital content is being created and distributed quickly. Therefore, people from academia are under pressure to create and study such content. In addition, there are a number of reading applications supporting different functions and diverse platforms, which distract the flow of research. To solve this problem, we created a prototype reading app, Researcher, for the tablet PC and mobile phone, which helps the flow of research by providing cooperation among platforms, seamlessly circulating between consumption and creation of contents, prioritizing contents by context, and holding attention by multimodal input. We conducted an in-depth interview and survey to verify the effectiveness of the features and to find out the appropriate modality of input for the flow of research.",Researcher: A Reading Application Helping the Flow of Research in Tablet and Mobile Phone,"Researcher is a mobile and tablet application for research that provides seamless change between consumption and creation of contents, prioritization of contents by context, and hold of attention by multimodal inputs.",,,,
int138,A,BandSense: Pressure-sensitive Multi-touch Interaction on a Wristband,Sungjae,Hwang,best@kaist.ac.kr,"Youngseok Ahn, Sungjae Hwang, Hyungook Yoon, Jung-hee Ryu","youngseok.ahn@futureplay.co, sungjae.hwang@futureplay.co, hyungook.yoon@futureplay.co, junghee.ryu@futureplay.co","In this paper, we propose a new interaction technique, called BandSense, which allows pressure-sensitive multi-touch interaction on a wristband. The proposed method provides users with a broader interaction area and higher input expressiveness, enabling a precision interaction with a less occlusion. To illustrate the potential of our approach, we present a series of example applications with several input vocabularies. We also describe the overall architecture of our system. We believe that our technique would greatly help users control a smartwatch easily and conveniently.",BandSense: Pressure-sensitive Multi-touch Interaction on a Wristband ,Experience a new interaction technique on a Wristband!,,,,
int139,A,Wearable Solution for Industrial Maintenance,Sam,Zheng,sam.zheng@gmail.com,"Sam Zheng, Patrik Matos, Cedric Foucault, Siddharth Dasari, Meng Yuan, Stuart Goose","sam.zheng@gmail.com, patrik.matos@siemens.com, cedric.foucault@siemens.com, siddharth.dasari@siemens.com, yuanmeng@siemens.com, stuart.goose@siemens.com","Wearable technology, such as Google Glass, offers potential benefits to engineers in industrial settings. We designed and developed a wearable solution for industrial maintenance, which 1) provides workflow guidance to the user, 2) supports hands-free operation, 3) allows the users to focus on their work, and 4) enables an efficient way for collaborating with a remote expert. The prototype, which was demonstrated at InnoTrans 2014, the largest international trade show for train technology, received positive feedback from many potential users and customers.",Wearable Solution for Industrial Maintenance ,"Wearable solution for industrial maintenance that provides workflow guidance, supports hands-free operation, allows the users to focus on their work, and enables an efficient way for remote collaboration. ",,,,
int143,A,Development of Realistic Digital Expression of Human Avatars through Pupillary Responses based on Heart Rate,Myoung Ju,Won,dnjsaudwn@naver.com,"Myoung Ju Won, Sangin Park, SungTeac Hwang, Mincheol Whang","dnjsaudwn@naver.com, ini0630@naver.com, columstyle@naver.com, whang@smu.ac.kr","The eyes of a virtual human avatar are the fundamental means for communicating language and affective feelings in a virtual environment. For this purpose, this study evaluates user's visual feeling according to the changes pupillary responses based on the heart rate for a representing a human avatar; this is considered as a new factor for representing a realistic avatar. We construct a human avatar in which the pupillary response is delivered real-time based on the heart rate. The results can be regarded as the basis for designing a realistic human avatar system by supporting a new visual realistic representing factor.",Development of Realistic Digital Expression of Human Avatars through Pupillary Responses based on Heart Rate.,"Pupillary response and Heart rate your virtual avatar, Experience a fun realistic digital expression human avatar with your friends.",2015-0029756,Global Frontier R&D Program (MSIP),,
int144,A,Waving Authentication: Your Smartphone Authenticate You on Motion Gesture,Feng,Hong,hongfeng@ouc.edu.cn,"Feng Hong, Meiyu Wei, Shujuan You, Yuan Feng, Zhongwen Guo","hongfeng@ouc.edu.cn, weimeiyu2012@gmail.com, youshujuan@gmail.com, fengyuan@ouc.edu.cn, guozhw@ouc.edu.cn","User authentication is important to protect sensitive and private information for smartphone users. \ We propose Waving Authentication (WA) which is a motion gesture authentication system based on accelerometer. WA utilizes eight distinguishing features hiding in the acceleration traces of motion gestures and exploits one-class Support Vector Machine for classification. It is insusceptible to shoulder surfing attacks. In the interactivity, we first \ provide two exhibitors' phones for audiences to try intruding WA by all kinds of waving. And we present our WA app to the audience smartphones, letting the phone to recognize their owners on audiences motion gesture.",Waving Authentication: Your Smartphone Authenticate You on Motion Gesture,"Waving Authentication exploits the unique features inside motion gesture for smartphone authentication. During the interactivity, you may watch motion gestures and ask how the exhibitor performs, but cannot imitate to unlock. ",61379128,NSFC,NSFC,61379127
int145,A,Data Transmission Method for Mobile Phone Using Groove Scan Code,JunBong,Song,junbong.song@gmail.com,"Junbong Song, Hyunwoo Bang","junbong.song@gmail.com, savoy@snu.ac.kr","A new mobile phone data-transmit method, groove scan code, is suggested for transmitting low-size data. As groove scan code uses audible collision sound generated by scanning on pre-encoded groove pattern as a data source, it can be implemented at minimum cost. By decoding the scanning sound, the original data can be transferred to mobile phone. Through the feasibility tests, we achieved an acceptable data decoding success rate and transmitting speed. The usability and intuitiveness are scored through user interviews.","Groove Scan Code, The new mobile phone data-transmit method","New mobile phone data-transmit method, groove scan code, is suggested for low-size data with low cost. Only with scanning encoded grooved pattern, encoded data transfers to mobile phone!!",,,,
int147,A,NOISA: A Novel Intelligent System Facilitating Smart Interaction,Koray,Tahiroğlu,koray.tahiroglu@aalto.fi,"Koray Tahiroğlu, Thomas Svedström, Valtteri Wikström","koray.tahiroglu@aalto.fi, thomas.svedstrom@aalto.fi, valtteri.wikstrom@aalto.fi","In this paper, we describe NOISA (Network of Intelligent Sonic Agents). NOISA is an intelligent system that acts to maintain and deepen the user’s engagement with digital artefacts by learning from the user’s actions and behavioural patterns in the moment of interaction. It facilitates a smart interaction by monitoring user’s bodily movements, facial expressions and control inputs. We present our model and system in a musical context, interfaced with our digital musical instrument (DMI). Our concept can be further extended to possible application areas in Human Computer Interaction (HCI) research field.",￼NOISA: A Novel Intelligent System Facilitating Smart Interaction,Experience a smart and engaging interaction with NOISA musical instruments !,Starting Grant,"Aalto University, School of ARTS",,
int148,A,Multi-Player Gaming on Spherical Displays,Julie,Williamson,julie@dcs.gla.ac.uk,"Julie R Williamson, John Williamson, Daniel Sundén, Jay Bradley","Julie.Williamson@glasgow.ac.uk, johnh.williamson@glasgow.ac.uk, daniel@sunden.se, jay@pufferfishdisplays.co.uk","Spherical displays offer unique affordances for multi-player games and playful interactions in social spaces.  The shape of a spherical display allows users to face each other and maintain eye contact during interaction, creating a different social dynamic than at a flat display.  There is also no intrinsically defined front or centre of the display, offering different views from different viewing angles. This creates shared and private areas of the display given users’ varying perspectives.  Trajectory based games have a dramatically different experience when played on a spherical surface.  Side-scrolling games are also exciting on a spherical surface, becoming “rotating” games where users’ action affect others playing at different points around the screen.  This Interactivity exhibit showcases two multi-player games that specifically exploit the affordances of a spherical display in a social setting.  ",Multi-Player Gaming on Spherical Displays,Spherical displays offer unique affordances for multi-player games and playful interactions.  This Interactivity exhibit showcases two multi-player games that specifically exploit the affordances of a spherical display in a social setting.,EP/M002675/1,EPSRC,,
int149,A,Sustainable Transport System: A Wheel Based Interactive Information Installation,GEON DONG,KIM,geon705@gmail.com,"Geon Dong Kim, Juhyun Eune","geon705@gmail.com, jheune@snu.ac.kr","Sustainable Transport System is an interactive wheel based information installation where users can watch the information projected on the wheel with a narration. Its information consists of 9 questions that are related with road transport systems ranging from 'The history of road traffic' to 'How will the sustainable traffic system evolve in the future?’ The circular interface was used to show information in a pie chart, diagram and history of wheel. This interface contains the meaning of sustainable circulation. The modalities of the project are Vision, Sonic, and Touch. A potentiometer sensor is mounted onto the center of wheel, which is linked with Flash action script through Arduino as the technical method. The user enables the information to be navigated by rotating the wheel clockwise from No.1 to No.9 and counterclockwise from No.9 to No.1. CHI attendees can experience the information ranging from history of road transport to necessity of sustainable transport system easily, interestingly and engagingly.",Sustainable Transport System: A Wheel Based Interactive Information Installation,"CHI attendees can experience the information ranging from history of road transport to necessity of sustainable transport system easily, interestingly and engagingly.",,,,
int150,A,Smart Eyewear for Interaction and Activity Recognition,Kai,Kunze,kai.kunze@gmail.com,"Shoya Ishimaru, Kai Kunze, Katsuma Tanaka, Yuji Uema, Koichi Kise, Masahiko Inami","ishimaru@m.cs.osakafu-u.ac.jp, kai.kunze@gmail.com, katsuma@m.cs.osakafu-u.ac.jp, uema@kmd.keio.ac.jp, kise@cs.osakafu-u.ac.jp, inami@inami.info","vice class with a lot of possibilities for user interac- tion design and unobtrusive activity tracking. In this paper we show applications using an early prototype of J!NS MEME, smart glasses with integrated electrodes to detect eye movements (Electrooculography, EOG) and motion sensors (accelerometer and gyroscope) to monitor head motions. We present several demonstrations: We show a simple eye movement visualization, detecting left/right eye motion and blink. Additionally, users can play a game, ’Blinky Bird’. They need to help a bird avoid obstacles using eye movements. We implemented online detection of reading and talking behavior using a combination of blink, eye movement and head motion. We can give people a long term view of their reading, talking, and also walking activity over the day.",Smart Eye Wear,"Smart Eye Wear is a novel device class with potential. We present applications using an early prototype of J!NS MEME, smart glasses that can detect head and eye movements. ",,,,
int151,A,Datawear: Self-reflection on the Go or How to Ethically Use Wearable Cameras for Research,Anya,Skatova,anya.skatova@gmail.com,"Anya Skatova, Victoria E Shipp, Lee Spacagna, Benjamin Bedwell, Ahmad Beltagui, Tom Rodden","anya.skatova@gmail.com, victoria.shipp@nottingham.ac.uk, lee.spacagna@nottingham.ac.uk, benjamin.bedwell@nottingham.ac.uk, a.beltagui@wlv.ac.uk, tar@Cs.Nott.AC.UK","A growing number of studies use wearable sensors, including cameras, to detect user activity patterns. When an object of academic investigation, these patterns are interpreted by researchers and conclusions are drawn about people’s habits and routines. Alternatively, interpretations are provided by users themselves during extensive post-study interviews. Such approaches inevitably expose personal data collected about individuals to researchers, which can potentially change the behavior under investigation. We introduce a new approach to using wearable sensor data in research. It allows people to interpret and self-reflect on their data and submit for investigation only reflections, without sharing their raw data. In this interactivity, we present and discuss the Datawear mobile application prototype, which is designed to conduct “in the wild” studies of personal experiences. ",Datawear,Experience a new form of data collection with a Datawear app. You can collect the images of your everyday experiences and provide reflections about them on the go.   ,EP/G065802/1,EPSRC UK,,
int152,A,TESSA - Toolkit for Experimentation  with Multimodal Sensory Substitution and Augmentation,Carlos,Sainz Martinez,c.sainzmartinez@pgr.reading.ac.uk,"Carlos Sainz Martinez, Faustina Hwang","c.sainzmartinez@pgr.reading.ac.uk, f.hwang@reading.ac.uk","TESSA is a toolkit for experimenting with sensory augmentation. It includes hardware and software to facilitate rapid prototyping of interfaces that can enhance one sense using information gathered from another sense. The toolkit contains a range of sensors (e.g. ultrasonics, temperature sensors) and actuators (e.g. tactors or stereo sound), designed modularly so that inputs and outputs can be easily swapped in and out and customized using TESSA’s graphical user interface (GUI), with “real time” feedback.  The system runs on a Raspberry Pi with a built-in touchscreen, providing a compact and portable form that is amenable for field trials.  At CHI Interactivity, the audience will have the opportunity to experience sensory augmentation effects using this system, and design their own sensory augmentation interfaces.",Toolkit for Experimentation with Sensory Substitution and Augmentation,"Experience Sensory Augmentation! With TESSA you can hear North, feel temperatures at a distance, or sense walls while blindfolded - and many more combinations that you can design and assemble yourself!",,,,
int154,A,WoBo: Multisensorial travels through Oculus Rift,Lucio Davide,Spano,davide.spano@unica.it,"Stefano Fibbi, Fabio Sorrentino, Lucio Davide Spano, Riccardo Scateni","stefano.fibbi@gmail.com, sorrefabio@gmail.com, davide.spano@unica.it, riccardo@unica.it","WoBo (World in a Box) aims to provide a new experience for travellers, allowing them to visit distant or hardly reachable places through the exploitation of consumer cameras and a head mounted display. The experience consists in watching a 360-degrees video with 3D audio in a dedicated cabin. The user can select videos shot in different places, which have been created with six consumer cameras. We describe the proposed experience, the hardware and the software used for a first prototype.",WoBo: Multisensorial travels through Oculus Rift,"Why looking for a genius to get it out of the box? Enter in WoBoX and go wherever you want, even without a genius!","POR FSE Axis IV, Obj l.3, LoA 1.3.1",Sardinia Regional Government,,
int155,A,Canvas Dance: An Interactive Dance Visualization for Large-Group Interaction,Carla,Griggio,carla.griggio@gmail.com,"Carla F Griggio, Mario Romero","carla.griggio@gmail.com, marior@kth.se","We present Canvas Dance, a prototype of an interactive dance visualization for large-group interaction that targets non-professional dancers in informal environments such as parties or nightclubs, and uses the smartphones of the dancers as the input device for the motion signal. The visualization is composed of individual representations for each dancer, and the visual mappings designed for their dance moves have three main goals: to help the users identify their own representation, to uncover and inspire imitation among dancers, and to support unpredictable dance moves. ",Canvas Dance,Download the Canvas Dance app to your phone and save it in a pocket. Dance with others to animate the visuals on the screen and create cool visual effects together!,,,,
int158,A,Nebula: An Interactive Garment Designed for Functional Aesthetics,Ludvig,Elblaus,elblaus@kth.se,"Ludvig Elblaus, Vasiliki Tsaknaki, Vincent Lewandowski, Roberto Bresin","elblaus@kth.se, tsaknaki@kth.se, vlew@kth.se, roberto@kth.se","In this paper we present Nebula, a prototype for examining the properties of textiles, fashion accessories, and digital technologies to arrive at a garment design that brings these elements together in a cohesive manner. Bridging the gap between everyday performativity and enactment, we aim at discussing aspects of the making process, interaction and functional aesthetics that emerged. \  \ Nebula is part of the Sound Clothes project that aims at exploring the expressive potential of wearable technologies creating sound from motion. \ ",Nebula: An Interactive Garment Designed for Functional Aesthetics,"Galactic sounds from a myriad of studs: The Nebula interactive garment reacts to the subtle movements of the wearer by creating expressive soundscapes, blurring the boundaries between fashion and performativity.",,,,
int159,A,Digiti Sonus v2: New Interface for Fingerprint Data Sonification using Hand Motion,Yoon Chung,Han,hanyoonjung@gmail.com,"Yoon Chung Han, Byeong-jun Han","yoon@mat.ucsb.edu, hbj1147@korea.ac.kr","Digiti Sonus v2 is a new interface for fingerprint data sonification using audience’s biometric data and apply their hand motions to control and modify their own audiovisual contents. This interface explores users’ fingerprint data as personalized artistic materials, and allows them to rearrange and compare the data with others as pieces of a sonic puzzle by hand motion. This work expands the possibility of creating diverse audiovisual results based on users’ interaction, and enhances easier and more intuitive interaction with hand motion using short-range depth camera.",Digiti Sonus v2: New Interface for Fingerprint Data Sonification using Hand Motion ,Digiti Sonus v2 is a new interface for fingerprint data sonification using audience’s biometric data and apply their hand motions to control and modify their own audiovisual contents. ,,,,
int164,A,The Art.CHI Gallery: An Embodied Iterative Curation Experience,Nic,Lupfer,nic@ecologylab.net,"Nic Lupfer, William A. Hamilton, Andrew Webb, Rhema Linder, Ernest Edmonds, Andruid Kerne","nic@ecologylab.net, bill@ecologylab.net, andrew@ecologylab.net, rhema@ecologylab.net, ernest@ernestedmonds.com, andruid@ecologylab.net","We present an exhibition of the 2015 Art.CHI Gallery as an embodied and iterative curation experience. We develop a method to represent the initial curated collection of interactive art works as a holistic exhibition. We explore how to physically manifest and present the exhibition at the CHI conference. Our exhibition will invite visitors to interact with the gallery as an embodied experience. They will browse the exhibit, as a whole, and view details of individual works. Participants will be encouraged to iteratively curate their own miniature gallery as they peruse. These curations will be made accessible online and shared through social media.",The Art.CHI Gallery: An Embodied Iterative Curation Experience,Experience the 2015 Art.CHI Gallery! Use your body to explore the digital exhibition. Design and share your own curations.,1247126,National Science Foundation,,
int165,A,Filteryedping: A Dwell-Free Eye Typing Technique,Diogo,Pedrosa,diogo@icmc.usp.br,"Diogo Pedrosa, Maria da Graça Pimentel, Khai N Truong","diogo@icmc.usp.br, mgp@icmc.usp.br, ktruong8@uncc.edu","The ability to type using eye gaze only is extremely important for individuals with a severe motor disability. To eye type, the user currently must sequentially gaze at letters in a virtual keyboard and dwell on each desired letter for a specific amount of time to input that key. Dwell-based eye typing has two possible drawbacks: unwanted input if the dwell threshold is too short or slow typing rates if the threshold is long. We demonstrate an eye typing technique, which does not require the user to dwell on the letters that she wants to input. Our method automatically filters out unwanted letters from the sequence of letters gazed at while typing a word. It ranks candidate words based on their length and frequency and presents them to the user for confirmation. Spell correction and support for typing words not in the corpus are also included.",Filteryedping: A Dwell-Free Eye Typing Technique,Write using only the movement of the eyes! Try our dwell-free eye typing technique and see how fast you can type!,2012/01510-0,FAPESP,CNPq,311659/2011-0
int166,A,VoroGraph: Visualization Tools for Epidemic Analysis,Cody,Dunne,cdunne@us.ibm.com,"Cody Dunne, Michael Muller, Nicola Perra, Mauro Martino","cdunne@us.ibm.com, michael_muller@us.ibm.com, n.perra@neu.edu, mmartino@us.ibm.com","Epidemiologists struggle to integrate complex information about the incidence and spread of disease, in relation to population density and other demographic conditions, at geographical scales ranging from global air travel down to local commuting. A partial solution overlays air travel as arcs above color-coded maps. However, commuting is not shown and it is often challenging to understand changing relationships due to the visual complexity arcs introduce. Moreover, when region sizes and shapes vary their color-codings become difficult to perceive. We introduce three visualizations which combine representations of population, movement, and disease spread at a local scale that is consistent with a zoomable global scale: (1) a map with commuting border encodings, (2) a centroidal Voronoi tessellation morphing technique, and (3) a meta-layout showing commuting alongside air travel. Our work provides mid-level abstractions that expert epidemiologists can use for insights into contagion.",VoroGraph: Visualization Tools for Epidemic Analysis,"Epidemiologists study the spread of disease, whether by air travel or commuting. VoroGraph helps them follow the outbreak by morphing standard maps in several ways. Come try it for yourself!",,,,
int167,A,"삶 (“Salm”, “To Live”): Gaze Reactive Typography Inspired by Ahn Sang-Soo",Monchu,Chen,monchu@uma.pt,"Monchu Chen, Bongkeum Jeong, Yoram I Chisik","monchu@uma.pt, bongkeum@gmail.com, ychisik@gmail.com","This research aims to explore the concept of gaze reactive typography, in which the design changes dynamically according to how audiences view it.  Inspired by the philosophy and styles of the famous Korean typographer Ahn Sang-Soo, we created an installation to showcase and exemplify relationships in four different levels between viewing behaviors and dynamic representations of typography.","삶 (“Salm”, “To Live”): Gaze Reactive Typography Inspired by Ahn Sang-Soo",Come to experience how typography could dynamically react to different viewing behaviors.,,,,
